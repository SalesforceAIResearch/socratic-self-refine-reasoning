{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a9cd71",
   "metadata": {},
   "source": [
    "### Extracting Result from log/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb113f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from experiment.utils import score_math\n",
    "\n",
    "dataset_names = [\"math_level5\", \"aime24\", \"aime25\", \"zebra_puzzles\", \"mini_sudoku\"]\n",
    "dataset_range = [\"0-681\", \"0-30\", \"0-30\", \"0-100\", \"0-100\"]\n",
    "\n",
    "## Extract the result from the log file\n",
    "model = \"gpt-5-mini\"\n",
    "root_path = \"log/{model_name}\".format(model_name=model)\n",
    "\n",
    "def calculate_accuracy(results, dataset_name):\n",
    "    return sum(x[\"score\"] for x in results) / len(results)\n",
    "\n",
    "print_log = []\n",
    "\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    print_log.append(\"-\"*100)\n",
    "    print_log.append(f\"{dataset_name}:\")\n",
    "    path = os.path.join(root_path, dataset_name)\n",
    "    for method in os.listdir(path):\n",
    "        method_path = os.path.join(path, os.path.join(method, dataset_range[i]))\n",
    "        accuracies = []\n",
    "        try:\n",
    "            for file_name in os.listdir(method_path):\n",
    "                if file_name.endswith(\".json\") and \"score\" not in file_name:\n",
    "                    with open(os.path.join(method_path, file_name), \"r\") as f:\n",
    "                        results = json.load(f)\n",
    "                        accuracies.append(calculate_accuracy(results, dataset_name))\n",
    "            # print_log.append(f\"{method}: {np.mean(accuracies)*100:.2f} ({np.std(accuracies)*100:.2f}), Support of Runs: {len(accuracies)}\")\n",
    "            print_log.append(f\"({len(accuracies)}) {method}: & {np.mean(accuracies)*100:.2f}\\\\scriptsize\" + \"{$\\\\pm$\" + f\"{np.std(accuracies)*100:.2f}\" + \"}\")\n",
    "        except Exception as e:\n",
    "            print_log.append(f\"{method}: No result\")\n",
    "\n",
    "for log in print_log:\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279ea4e",
   "metadata": {},
   "source": [
    "#### Self-Consistency Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_log = []\n",
    "from experiment.utils import score_math, score_mc, majority_vote_answers, sudoku_match_score, exact_match_score\n",
    "import random\n",
    "\n",
    "method_names_for_sc = [\"cot\", \"self-refine\", \"atom\", \"mctsr\", \"ssr\", \"ssr-adaptive\", \"ssr-planning\"]\n",
    "dataset_names = [\"math_level5\", \"aime24\", \"aime25\", \"zebra_puzzles\", \"mini_sudoku\"]\n",
    "dataset_range = [\"0-681\", \"0-30\", \"0-30\", \"0-100\", \"0-100\"]\n",
    "\n",
    "def reasoning_gym_score(prediction, ground_truth, dataset_name):\n",
    "    if dataset_name == \"mini_sudoku\":\n",
    "        return sudoku_match_score(prediction, ground_truth)\n",
    "    elif dataset_name == \"zebra_puzzles\":\n",
    "        return exact_match_score(prediction, ground_truth)\n",
    "\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    if dataset_name == \"cryptarithm\":\n",
    "        def majority_vote_function(answers):\n",
    "            def extract_dict(answer):\n",
    "                return {k.strip(): v.strip() for k, v in [row.split(\"=\")[:2] for row in answer.split(\",\") if \"=\" in row]}\n",
    "            # a majorigy vote function for the extracted dict\n",
    "            # two dicts are the same if they have the same keys and values\n",
    "            # majority vote for the extracted dict\n",
    "            def majority_vote_dict(dicts):\n",
    "                from collections import Counter\n",
    "                # Convert dicts to tuples of sorted items for counting\n",
    "                dict_tuples = [tuple(sorted(d.items())) for d in dicts]\n",
    "                most_common_tuple, _ = Counter(dict_tuples).most_common(1)[0]\n",
    "                # Convert the tuple back to dict\n",
    "                return {tup[0]: tup[1] for tup in most_common_tuple}\n",
    "            def dict_to_string(dict):\n",
    "                return \",\".join([f\"{k}={v}\" for k, v in dict.items()])\n",
    "            return dict_to_string(majority_vote_dict([extract_dict(answer) for answer in answers]))\n",
    "    else:\n",
    "        majority_vote_function = majority_vote_answers\n",
    "    print_log.append(\"-\"*100)\n",
    "    print_log.append(f\"{dataset_name}:\")\n",
    "    path = os.path.join(root_path, dataset_name)\n",
    "    if dataset_name == \"gpqa\":\n",
    "        score_function = score_mc\n",
    "    elif dataset_name in [\"cryptarithm\", \"mini_sudoku\", \"zebra_puzzles\"]:\n",
    "        score_function = reasoning_gym_score\n",
    "    else:\n",
    "        score_function = score_math\n",
    "    for method in method_names_for_sc:\n",
    "        method_path = os.path.join(path, os.path.join(method, dataset_range[i]))\n",
    "\n",
    "        try:\n",
    "            rollouts = []\n",
    "            for file_name in os.listdir(method_path):\n",
    "                if file_name.endswith(\".json\") and \"score\" not in file_name:\n",
    "                    with open(os.path.join(method_path, file_name), \"r\") as f:\n",
    "                        results = json.load(f)\n",
    "                        # accuracies.append(calculate_accuracy_sc(results, dataset_name))\n",
    "                        rollouts.append(results)\n",
    "            # bootstrapping 5 results from rollouts and calculate the accuracy and std\n",
    "            accuracies = []\n",
    "            for _ in range(50):\n",
    "                bootstrap_rollouts = random.sample(rollouts, 5)\n",
    "                correct, total = 0, 0\n",
    "                for j in range(len(bootstrap_rollouts[0])):\n",
    "                    gt = bootstrap_rollouts[0][j][\"groundtruth\"]\n",
    "                    answers = [rollout[j][\"answer\"] for rollout in bootstrap_rollouts]\n",
    "                    final_answer = majority_vote_function(answers)\n",
    "                    if dataset_name == \"gpqa\":\n",
    "                        if score_mc(final_answer, gt) == 1:\n",
    "                            correct += 1\n",
    "                    else: \n",
    "                        correct += score_function(final_answer, gt, dataset_name)\n",
    "                    total += 1\n",
    "                accuracies.append(correct/total)\n",
    "            print_log.append(f\"({len(accuracies)}) {method}: & {np.mean(accuracies)*100:.2f}\\\\scriptsize\" + \"{$\\\\pm$\" + f\"{np.std(accuracies)*100:.2f}\" + \"}\")\n",
    "\n",
    "\n",
    "        # print_log.append(f\"{method}: {(correct / total)*100:.2f} ({0:.2f}), Support of Runs: {len(accuracies)}\")\n",
    "        except Exception as e:\n",
    "            print(\"Error: \", e)\n",
    "            print_log.append(f\"{method}: No result\")\n",
    "\n",
    "for log in print_log:\n",
    "    print(log)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
